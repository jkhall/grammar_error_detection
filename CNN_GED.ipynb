{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_GED.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkhall/grammar_error_detection/blob/master/CNN_GED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "lNssCi4kBaCg",
        "colab_type": "code",
        "outputId": "f92e9474-5b07-4fe0-f92f-af2b1ff069d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RvH4NSaALcZ1",
        "colab_type": "code",
        "outputId": "9c4ab43a-112e-4786-bd07-01452f51ad2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "!ls /gdrive/My\\ Drive/data\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'aesw2016(v1.2)_train.xml'\t glove.6B.300d.txt\n",
            "'aesw2016(v1.2)_train.xml.bz2'\t glove.6B.50d.txt\n",
            " glove.6B.100d.txt\t\t GoogleNews-vectors-negative300.bin\n",
            " glove.6B.200d.txt\t\t tarantino.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y3qxjjpiBOFN",
        "colab_type": "code",
        "outputId": "7eccf18e-ff62-4998-8bc3-9c21588594fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip /gdrive/My\\ Drive/data/glove\\.6B\\.zip -d /gdrive/My\\ Drive/data/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /gdrive/My Drive/data/glove.6B.zip\n",
            "  inflating: /gdrive/My Drive/data/glove.6B.50d.txt  \n",
            "  inflating: /gdrive/My Drive/data/glove.6B.100d.txt  \n",
            "  inflating: /gdrive/My Drive/data/glove.6B.200d.txt  \n",
            "  inflating: /gdrive/My Drive/data/glove.6B.300d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ja-QFuloo9a7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gzip -dk /gdrive/My\\ Drive/data/GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XIPijHC-Ov97",
        "colab_type": "code",
        "outputId": "1da17fc5-eb0a-4241-b377-83950250182e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "\n",
        "tree = ET.parse('/gdrive/My Drive/data/aesw2016(v1.2)_train.xml')\n",
        "root = tree.find('training')\n",
        "\n",
        "# labels -> 0 = no error, 1 = error\n",
        "\n",
        "par = root.find('par')\n",
        "sentence = par.findall('sentence')\n",
        "\n",
        "inputs = []\n",
        "labels = []\n",
        "\n",
        "def clean(src):\n",
        "#   ins = src.iterfind('ins')\n",
        "#   dels = src.findall('del')\n",
        "  \n",
        "  itr = src.iter()\n",
        "  \n",
        "  txt = \"\"\n",
        "  \n",
        "  if src.text != None:\n",
        "    txt += src.text\n",
        " \n",
        "  for tag in itr:\n",
        "    if tag.tag == 'del':\n",
        "      if tag.text != None:\n",
        "        txt += tag.text\n",
        "    \n",
        "    if tag.tail != None:\n",
        "      txt += tag.tail\n",
        "\n",
        "  txt = re.sub('\\n\\t', \" \", txt)\n",
        "  txt = re.sub(\"_\", \"\", txt)\n",
        "  return txt\n",
        "  \n",
        "\n",
        "# err = sentence[1].find('del') == None #<-- this is how we check if the sentence contains errors\n",
        "\n",
        "# create labels and inputs\n",
        "for par in root.findall('par'):\n",
        "  for sentence in par.findall('sentence'):\n",
        "    inputs.append(clean(sentence))\n",
        "    \n",
        "    if sentence.find('del') != None or sentence.find('ins') != None:\n",
        "      labels.append(1)\n",
        "    else:\n",
        "      labels.append(0)\n",
        "      \n",
        "\n",
        "\n",
        "print(inputs[2])\n",
        "print(labels[2])\n",
        "    \n",
        "  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is important since only MATH is the real, ordering parameter, while MATH is complex and thus has no natural ordering.  \n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xmWa8bfPhoB-",
        "colab_type": "code",
        "outputId": "25587b76-b8be-43ee-f854-e517a2e90c5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "s = sum(labels)\n",
        "sentence_lengths = [len(sent.split()) for sent in inputs]\n",
        "sl_total = sum(sentence_lengths)\n",
        "\n",
        "\n",
        "print(\"Percentage of sentences with errors: {}\".format(s / len(labels)))\n",
        "print(\"Num examples: {}\".format(len(inputs)))\n",
        "print(\"Average sentence length: {}\".format(sl_total / len(inputs)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of sentences with errors: 0.3923552141730536\n",
            "Num examples: 1189412\n",
            "Average sentence length: 20.858449385074305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YK1f9iArj8i3",
        "colab_type": "code",
        "outputId": "9d3c6bd5-5d06-4221-8353-9a4d7304ab3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "t = \"hey,\"\n",
        "thing = \"how do you do\"\n",
        "punc = \"[!:;,.?]\"\n",
        "cnt = 7\n",
        "arr = [1,2,3]\n",
        "a = arr\n",
        "cnt += 1\n",
        "arr.append(4)\n",
        "b = arr\n",
        "# print(\"a: {}, b: {}\".format(a, b))\n",
        "\n",
        "print(\"in\" in \"print\")\n",
        "\n",
        "re.sub(punc, \"\", t) == t"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "6qI5EDZEh7YF",
        "colab_type": "code",
        "outputId": "291dd964-2fbc-4196-83ea-dc9a934eb021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "# from tensorflow import keras\n",
        "from keras.preprocessing import sequence, text\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Preprocessing\n",
        "punc = '!?;:,.'\n",
        "\n",
        "num_examples = 500000\n",
        "\n",
        "# unclear how sensitive to vocab_size the model is\n",
        "vocab_size = 10000\n",
        "\n",
        "max_len = 40\n",
        "\n",
        "# should get the most common words\n",
        "# how should I deal with punctuation? Throw it all out? Try to keep it as a word? Embed it?\n",
        "# can I have just a general <punc> tag? (eh)\n",
        "\n",
        "def build_word_ids(sentences, vocabsize=1000):\n",
        "  d = {}\n",
        "  for s in sentences:\n",
        "    words = s.split()\n",
        "    for w in words:\n",
        "      if w in d:\n",
        "        d[w] += 1\n",
        "      else:\n",
        "        d[w] = 1\n",
        "      \n",
        "  sorted_d = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
        "  \n",
        "  w_ids = {}\n",
        "  cnt = 1\n",
        "  \n",
        "  for k, v in sorted_d[:vocabsize]:\n",
        "   \n",
        "    w_ids[k] = cnt \n",
        "    cnt += 1\n",
        "\n",
        "  return w_ids\n",
        "\n",
        "word_ids = build_word_ids(inputs, vocabsize=vocab_size)\n",
        "word_ids[\"<UKN>\"] = vocab_size\n",
        "vocab_size += 1\n",
        "word_ids[\"!\"] = vocab_size\n",
        "vocab_size += 1\n",
        "word_ids[\"?\"] = vocab_size\n",
        "vocab_size += 1\n",
        "word_ids[\";\"] = vocab_size\n",
        "vocab_size += 1\n",
        "word_ids[\":\"] = vocab_size\n",
        "vocab_size += 1\n",
        "word_ids[\",\"] = vocab_size\n",
        "vocab_size += 2 # for the padding character\n",
        "\n",
        "processed_inp = []\n",
        "\n",
        "for sentence in inputs:\n",
        "  words = sentence.split()\n",
        "  new_sent = []\n",
        "  for w in words:\n",
        "    for p in punc:\n",
        "      if p in w:\n",
        "        new_sent.append(word_ids[p])\n",
        "      w = re.sub(punc, \"\", w)\n",
        "      break\n",
        "    if w in word_ids:\n",
        "      new_sent.append(word_ids[w])\n",
        "    else:\n",
        "      new_sent.append(word_ids[\"<UKN>\"])\n",
        "      \n",
        "  processed_inp.append(new_sent)\n",
        "\n",
        "# pad sequence with unused id: vocab_size - 2\n",
        "processed_inp = sequence.pad_sequences(processed_inp, max_len)\n",
        "  \n",
        "X = np.array(processed_inp[:num_examples])\n",
        "y = np.array(labels[:num_examples])\n",
        "\n",
        "# shuffle\n",
        "indices = np.arange(y.shape[0])\n",
        "indices = np.random.shuffle(indices)\n",
        "\n",
        "X = np.squeeze(X[indices])\n",
        "y = np.squeeze(y[indices])\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "train_split = int(num_examples * 0.8)\n",
        "# test_split = num_examples * 0.2\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = (X[:train_split], y[:train_split]), (X[train_split:], y[train_split:])\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(500000, 40)\n",
            "(500000,)\n",
            "(400000, 40)\n",
            "(100000, 40)\n",
            "(400000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XcfPk7xxE2ET",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "embedding_size = 300\n",
        "embeddings1 = {}\n",
        "embeddings2 = {}\n",
        "\n",
        "glovePath = '/gdrive/My Drive/data/glove.6B.' + str(embedding_size) + 'd.txt'\n",
        "word2VecPath = '/gdrive/My Drive/data/GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "embeddings2 = KeyedVectors.load_word2vec_format('/gdrive/My Drive/data/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "# word_vectors.save_word2vec_format('/gdrive/My Drive/data/GoogleNews-vectors-negative300.txt', binary=False)\n",
        "\n",
        "\n",
        "f = open(glovePath)\n",
        "for line in f:\n",
        "  vals = line.split()\n",
        "  word = vals[0]\n",
        "  dimens = np.asarray(vals[1:], dtype='float32')\n",
        "  embeddings1[word] = dimens\n",
        "  \n",
        "f.close()\n",
        "\n",
        "embedding_mat1 = np.zeros((vocab_size, embedding_size))\n",
        "embedding_mat2 = np.zeros((vocab_size, embedding_size))\n",
        "\n",
        "for w, i in word_ids.items(): \n",
        "  # what else might we initialize absent words to, instead of zero?\n",
        "  if w in embeddings1:\n",
        "    embedding_mat1[i] = embeddings1[w]\n",
        "    \n",
        "for w, i in word_ids.items(): \n",
        "  # what else might we initialize absent words to, instead of zero?\n",
        "  if w in embeddings2:\n",
        "    embedding_mat2[i] = embeddings2[w]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eTFGh5VmO7If",
        "colab_type": "code",
        "outputId": "5c189f0a-3efe-4e3b-f5f4-9ec7f05ccb5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    0     0     0     0     0   127  4724    26  3176  1178  1941     1\n",
            "   216    14   251 15007     4  2319   149    10   300    51  1828    11\n",
            "    32    30    19  1175     3    10     3     4   752  1993   167     3\n",
            "     4     3    10    12]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZvjzSdLoSaNN",
        "colab_type": "code",
        "outputId": "f0356c17-4984-4500-c361-8e97ff96d9a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential, Model\n",
        "\n",
        "filter_sizes = [3,4,5]\n",
        "\n",
        "inputs = layers.Input(shape=(max_len,))\n",
        "emb1 = layers.Embedding(vocab_size, embedding_size, weights=[embedding_mat1], trainable=True)(inputs)\n",
        "emb2 = layers.Embedding(vocab_size, embedding_size, weights=[embedding_mat2], trainable=False)(inputs)\n",
        "c_emb = layers.concatenate([emb1, emb2])\n",
        "# emb = layers.Embedding(vocab_size, embedding_size)(inputs)\n",
        "conv1 = layers.Conv1D(128, filter_sizes[0], activation='relu')(c_emb)\n",
        "pool1 = layers.MaxPool1D(max_len - filter_sizes[0] + 1)(conv1)\n",
        "conv2 = layers.Conv1D(128, filter_sizes[1],activation='relu')(c_emb)\n",
        "pool2 = layers.MaxPool1D(max_len - filter_sizes[1] + 1)(conv2)\n",
        "conv3 = layers.Conv1D(128, filter_sizes[2],activation='relu')(c_emb)\n",
        "pool3 = layers.MaxPool1D(max_len - filter_sizes[2] + 1)(conv3)\n",
        "merged = layers.concatenate([pool1, pool2, pool3])\n",
        "glo_pool = layers.GlobalMaxPooling1D()(merged)\n",
        "drop = layers.Dropout(0.2)(glo_pool)\n",
        "score = layers.Dense(1, activation='sigmoid')(drop)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=score)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "         epochs=2,\n",
        "         batch_size=100,\n",
        "         validation_split=0.2)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 40)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 40, 300)      3002100     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 40, 300)      3002100     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 40, 600)      0           embedding_3[0][0]                \n",
            "                                                                 embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 38, 128)      230528      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 37, 128)      307328      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 36, 128)      384128      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 1, 128)       0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 1, 128)       0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 1, 128)       0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 1, 384)       0           max_pooling1d_4[0][0]            \n",
            "                                                                 max_pooling1d_5[0][0]            \n",
            "                                                                 max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 384)          0           concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 384)          0           global_max_pooling1d_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            385         dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 6,926,569\n",
            "Trainable params: 3,924,469\n",
            "Non-trainable params: 3,002,100\n",
            "__________________________________________________________________________________________________\n",
            "Train on 320000 samples, validate on 80000 samples\n",
            "Epoch 1/2\n",
            "320000/320000 [==============================] - 117s 367us/step - loss: 0.5663 - acc: 0.7010 - val_loss: 0.5570 - val_acc: 0.7121\n",
            "Epoch 2/2\n",
            "320000/320000 [==============================] - 117s 365us/step - loss: 0.5294 - acc: 0.7299 - val_loss: 0.5381 - val_acc: 0.7213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6Owd0jyxyUgr",
        "colab_type": "code",
        "outputId": "1db39d5d-3aef-439c-9170-d228930aadf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        }
      },
      "cell_type": "code",
      "source": [
        "import keras \n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_size, weights=[embedding_mat], trainable=False))\n",
        "model.add(layers.LSTM(30, return_sequences=True))\n",
        "model.add(layers.LSTM(30, return_sequences=True, go_backwards=True))\n",
        "model.add(layers.LSTM(30))\n",
        "model.add(layers.Dropout(0.6))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                   epochs=10,\n",
        "                   batch_size=100,\n",
        "                   validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, None, 100)         2000000   \n",
            "_________________________________________________________________\n",
            "lstm_13 (LSTM)               (None, None, 30)          15720     \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (None, None, 30)          7320      \n",
            "_________________________________________________________________\n",
            "lstm_15 (LSTM)               (None, 30)                7320      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 30)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 31        \n",
            "=================================================================\n",
            "Total params: 2,030,391\n",
            "Trainable params: 30,391\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n",
            "Train on 32000 samples, validate on 8000 samples\n",
            "Epoch 1/10\n",
            "32000/32000 [==============================] - 35s 1ms/step - loss: 0.6441 - acc: 0.6353 - val_loss: 0.6291 - val_acc: 0.6469\n",
            "Epoch 2/10\n",
            "32000/32000 [==============================] - 31s 962us/step - loss: 0.6144 - acc: 0.6640 - val_loss: 0.5968 - val_acc: 0.6722\n",
            "Epoch 3/10\n",
            "32000/32000 [==============================] - 31s 970us/step - loss: 0.5962 - acc: 0.6753 - val_loss: 0.5968 - val_acc: 0.6649\n",
            "Epoch 4/10\n",
            "32000/32000 [==============================] - 31s 955us/step - loss: 0.5897 - acc: 0.6796 - val_loss: 0.5874 - val_acc: 0.6741\n",
            "Epoch 5/10\n",
            "32000/32000 [==============================] - 31s 955us/step - loss: 0.5866 - acc: 0.6845 - val_loss: 0.5846 - val_acc: 0.6811\n",
            "Epoch 6/10\n",
            "32000/32000 [==============================] - 31s 957us/step - loss: 0.5823 - acc: 0.6888 - val_loss: 0.5909 - val_acc: 0.6711\n",
            "Epoch 7/10\n",
            "32000/32000 [==============================] - 30s 942us/step - loss: 0.5791 - acc: 0.6904 - val_loss: 0.5920 - val_acc: 0.6744\n",
            "Epoch 8/10\n",
            "32000/32000 [==============================] - 30s 939us/step - loss: 0.5730 - acc: 0.6933 - val_loss: 0.5854 - val_acc: 0.6768\n",
            "Epoch 9/10\n",
            "32000/32000 [==============================] - 30s 944us/step - loss: 0.5695 - acc: 0.6977 - val_loss: 0.5853 - val_acc: 0.6809\n",
            "Epoch 10/10\n",
            "32000/32000 [==============================] - 31s 954us/step - loss: 0.5669 - acc: 0.6983 - val_loss: 0.5977 - val_acc: 0.6765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j2XRWPsCcf84",
        "colab_type": "code",
        "outputId": "0fbaa906-fed8-43f0-8000-99de021667b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1059
        }
      },
      "cell_type": "code",
      "source": [
        "import keras \n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_size, weights=[embedding_mat], trainable=False))\n",
        "model.add(layers.LSTM(32, return_sequences=True))\n",
        "model.add(layers.LSTM(32, return_sequences=True))\n",
        "model.add(layers.LSTM(32))\n",
        "model.add(layers.Dropout(0.6))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                   epochs=20,\n",
        "                   batch_size=100,\n",
        "                   validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, None, 100)         2000000   \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, None, 32)          17024     \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, None, 32)          8320      \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,033,697\n",
            "Trainable params: 33,697\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n",
            "Train on 32000 samples, validate on 8000 samples\n",
            "Epoch 1/20\n",
            "32000/32000 [==============================] - 32s 989us/step - loss: 0.6455 - acc: 0.6325 - val_loss: 0.6450 - val_acc: 0.6330\n",
            "Epoch 2/20\n",
            "32000/32000 [==============================] - 28s 873us/step - loss: 0.6246 - acc: 0.6554 - val_loss: 0.6101 - val_acc: 0.6710\n",
            "Epoch 3/20\n",
            "32000/32000 [==============================] - 28s 869us/step - loss: 0.6031 - acc: 0.6725 - val_loss: 0.5925 - val_acc: 0.6765\n",
            "Epoch 4/20\n",
            "32000/32000 [==============================] - 28s 869us/step - loss: 0.5933 - acc: 0.6770 - val_loss: 0.5881 - val_acc: 0.6809\n",
            "Epoch 5/20\n",
            "32000/32000 [==============================] - 28s 874us/step - loss: 0.5877 - acc: 0.6827 - val_loss: 0.5867 - val_acc: 0.6801\n",
            "Epoch 6/20\n",
            "32000/32000 [==============================] - 28s 876us/step - loss: 0.5840 - acc: 0.6883 - val_loss: 0.5839 - val_acc: 0.6823\n",
            "Epoch 7/20\n",
            "32000/32000 [==============================] - 28s 874us/step - loss: 0.5811 - acc: 0.6893 - val_loss: 0.5830 - val_acc: 0.6856\n",
            "Epoch 8/20\n",
            "32000/32000 [==============================] - 28s 887us/step - loss: 0.5764 - acc: 0.6913 - val_loss: 0.5856 - val_acc: 0.6825\n",
            "Epoch 9/20\n",
            "32000/32000 [==============================] - 28s 871us/step - loss: 0.5744 - acc: 0.6971 - val_loss: 0.5865 - val_acc: 0.6788\n",
            "Epoch 10/20\n",
            "32000/32000 [==============================] - 28s 871us/step - loss: 0.5683 - acc: 0.7006 - val_loss: 0.5826 - val_acc: 0.6821\n",
            "Epoch 11/20\n",
            "32000/32000 [==============================] - 28s 874us/step - loss: 0.5664 - acc: 0.7036 - val_loss: 0.5879 - val_acc: 0.6791\n",
            "Epoch 12/20\n",
            "32000/32000 [==============================] - 28s 879us/step - loss: 0.5608 - acc: 0.7062 - val_loss: 0.5971 - val_acc: 0.6736\n",
            "Epoch 13/20\n",
            "32000/32000 [==============================] - 28s 878us/step - loss: 0.5562 - acc: 0.7120 - val_loss: 0.5985 - val_acc: 0.6712\n",
            "Epoch 14/20\n",
            "32000/32000 [==============================] - 28s 879us/step - loss: 0.5512 - acc: 0.7162 - val_loss: 0.5920 - val_acc: 0.6734\n",
            "Epoch 15/20\n",
            "32000/32000 [==============================] - 28s 879us/step - loss: 0.5456 - acc: 0.7216 - val_loss: 0.5975 - val_acc: 0.6741\n",
            "Epoch 16/20\n",
            "32000/32000 [==============================] - 28s 876us/step - loss: 0.5386 - acc: 0.7256 - val_loss: 0.5972 - val_acc: 0.6670\n",
            "Epoch 17/20\n",
            "32000/32000 [==============================] - 28s 876us/step - loss: 0.5342 - acc: 0.7292 - val_loss: 0.6032 - val_acc: 0.6743\n",
            "Epoch 18/20\n",
            "32000/32000 [==============================] - 28s 875us/step - loss: 0.5284 - acc: 0.7344 - val_loss: 0.6121 - val_acc: 0.6731\n",
            "Epoch 19/20\n",
            "32000/32000 [==============================] - 28s 889us/step - loss: 0.5213 - acc: 0.7375 - val_loss: 0.6247 - val_acc: 0.6744\n",
            "Epoch 20/20\n",
            "32000/32000 [==============================] - 28s 877us/step - loss: 0.5157 - acc: 0.7419 - val_loss: 0.6356 - val_acc: 0.6677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nozpBIkOVRtn",
        "colab_type": "code",
        "outputId": "47b6b8cb-32b9-49df-a4f4-e4b9f7b13134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_size, weights=[embedding_mat1], trainable=False))\n",
        "model.add(layers.LSTM(128))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                   epochs=10,\n",
        "                   batch_size=100,\n",
        "                   validation_split=0.2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, None, 300)         3002100   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               219648    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 3,221,877\n",
            "Trainable params: 219,777\n",
            "Non-trainable params: 3,002,100\n",
            "_________________________________________________________________\n",
            "Train on 320000 samples, validate on 80000 samples\n",
            "Epoch 1/10\n",
            "320000/320000 [==============================] - 401s 1ms/step - loss: 0.5929 - acc: 0.6770 - val_loss: 0.5748 - val_acc: 0.6949\n",
            "Epoch 2/10\n",
            "320000/320000 [==============================] - 396s 1ms/step - loss: 0.5707 - acc: 0.6961 - val_loss: 0.5649 - val_acc: 0.7032\n",
            "Epoch 3/10\n",
            "320000/320000 [==============================] - 399s 1ms/step - loss: 0.5614 - acc: 0.7046 - val_loss: 0.5661 - val_acc: 0.7018\n",
            "Epoch 4/10\n",
            "320000/320000 [==============================] - 399s 1ms/step - loss: 0.5525 - acc: 0.7122 - val_loss: 0.5627 - val_acc: 0.7066\n",
            "Epoch 5/10\n",
            "320000/320000 [==============================] - 400s 1ms/step - loss: 0.5432 - acc: 0.7194 - val_loss: 0.5614 - val_acc: 0.7065\n",
            "Epoch 6/10\n",
            "320000/320000 [==============================] - 392s 1ms/step - loss: 0.5323 - acc: 0.7277 - val_loss: 0.5642 - val_acc: 0.7057\n",
            "Epoch 7/10\n",
            "320000/320000 [==============================] - 392s 1ms/step - loss: 0.5203 - acc: 0.7360 - val_loss: 0.5705 - val_acc: 0.7030\n",
            "Epoch 8/10\n",
            "320000/320000 [==============================] - 394s 1ms/step - loss: 0.5083 - acc: 0.7452 - val_loss: 0.5839 - val_acc: 0.6971\n",
            "Epoch 9/10\n",
            "320000/320000 [==============================] - 393s 1ms/step - loss: 0.4936 - acc: 0.7539 - val_loss: 0.5917 - val_acc: 0.6988\n",
            "Epoch 10/10\n",
            "320000/320000 [==============================] - 388s 1ms/step - loss: 0.4784 - acc: 0.7642 - val_loss: 0.5942 - val_acc: 0.6937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uaLdFIgoBun3",
        "colab_type": "code",
        "outputId": "8cc3e76b-a9b2-4505-d72d-50867fe465bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_size, weights=[embedding_mat1], trainable=False))\n",
        "model.add(layers.Conv1D(64, 3, activation='relu'))\n",
        "model.add(layers.Conv1D(64, 3, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(2))\n",
        "model.add(layers.Conv1D(64, 3, activation='relu'))\n",
        "model.add(layers.Conv1D(64, 3, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=RMSprop(lr=1e-4),\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['acc'])\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                   epochs=10,\n",
        "                   batch_size=100,\n",
        "                   validation_split=0.2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, None, 300)         3002100   \n",
            "_________________________________________________________________\n",
            "conv1d_11 (Conv1D)           (None, None, 64)          57664     \n",
            "_________________________________________________________________\n",
            "conv1d_12 (Conv1D)           (None, None, 64)          12352     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1 (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_13 (Conv1D)           (None, None, 64)          12352     \n",
            "_________________________________________________________________\n",
            "conv1d_14 (Conv1D)           (None, None, 64)          12352     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_4 (Glob (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 3,096,885\n",
            "Trainable params: 94,785\n",
            "Non-trainable params: 3,002,100\n",
            "_________________________________________________________________\n",
            "Train on 320000 samples, validate on 80000 samples\n",
            "Epoch 1/10\n",
            "320000/320000 [==============================] - 42s 130us/step - loss: 0.6339 - acc: 0.6463 - val_loss: 0.5949 - val_acc: 0.6822\n",
            "Epoch 2/10\n",
            "320000/320000 [==============================] - 46s 143us/step - loss: 0.5954 - acc: 0.6806 - val_loss: 0.5824 - val_acc: 0.6918\n",
            "Epoch 3/10\n",
            "320000/320000 [==============================] - 42s 130us/step - loss: 0.5850 - acc: 0.6886 - val_loss: 0.5837 - val_acc: 0.6894\n",
            "Epoch 4/10\n",
            "320000/320000 [==============================] - 40s 126us/step - loss: 0.5783 - acc: 0.6951 - val_loss: 0.5754 - val_acc: 0.6978\n",
            "Epoch 5/10\n",
            "320000/320000 [==============================] - 43s 133us/step - loss: 0.5720 - acc: 0.7006 - val_loss: 0.5753 - val_acc: 0.6978\n",
            "Epoch 6/10\n",
            "320000/320000 [==============================] - 44s 138us/step - loss: 0.5666 - acc: 0.7054 - val_loss: 0.5776 - val_acc: 0.6957\n",
            "Epoch 7/10\n",
            "320000/320000 [==============================] - 40s 125us/step - loss: 0.5617 - acc: 0.7102 - val_loss: 0.5820 - val_acc: 0.6898\n",
            "Epoch 8/10\n",
            "320000/320000 [==============================] - 40s 125us/step - loss: 0.5564 - acc: 0.7155 - val_loss: 0.5762 - val_acc: 0.6973\n",
            "Epoch 9/10\n",
            "320000/320000 [==============================] - 43s 135us/step - loss: 0.5512 - acc: 0.7200 - val_loss: 0.5807 - val_acc: 0.6945\n",
            "Epoch 10/10\n",
            "320000/320000 [==============================] - 43s 133us/step - loss: 0.5457 - acc: 0.7241 - val_loss: 0.5866 - val_acc: 0.6886\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}